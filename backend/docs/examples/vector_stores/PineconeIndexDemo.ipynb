{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
            "metadata": {},
            "source": [
                "# Pinecone Vector Store"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d48af8e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import sys\n",
                "import os\n",
                "\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
                "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
            "metadata": {},
            "source": [
                "#### Creating a Pinecone Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from tqdm.autonotebook import tqdm\n"
                    ]
                }
            ],
            "source": [
                "import pinecone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee",
            "metadata": {},
            "outputs": [],
            "source": [
                "api_key = os.environ['PINECONE_API_KEY']\n",
                "pinecone.init(api_key=api_key, environment=\"eu-west1-gcp\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88",
            "metadata": {},
            "outputs": [],
            "source": [
                "# dimensions are for text-embedding-ada-002\n",
                "pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "pinecone_index = pinecone.Index(\"quickstart-index\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
            "metadata": {},
            "source": [
                "#### Load documents, build the PineconeVectorStore and GPTVectorStoreIndex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "0a2bcc07",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                        "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                        "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                        "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
                        "NumExpr defaulting to 8 threads.\n",
                        "NumExpr defaulting to 8 threads.\n"
                    ]
                }
            ],
            "source": [
                "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
                "from llama_index.vector_stores import PineconeVectorStore\n",
                "from IPython.display import Markdown, display"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
            "metadata": {},
            "outputs": [],
            "source": [
                "# load documents\n",
                "documents = SimpleDirectoryReader('../data/paul_graham').load_data()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "ba1558b3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
                        "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n"
                    ]
                }
            ],
            "source": [
                "# initialize without metadata filter\n",
                "from llama_index.storage.storage_context import StorageContext\n",
                "\n",
                "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
                "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
                "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "04304299-fc3e-40a0-8600-f50c3292767e",
            "metadata": {},
            "source": [
                "#### Query Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "35369eda",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "> [retrieve] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
                        "> [retrieve] Total embedding token usage: 8 tokens\n",
                        "> [retrieve] Total embedding token usage: 8 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1917 tokens\n",
                        "> [get_response] Total LLM token usage: 1917 tokens\n",
                        "> [get_response] Total LLM token usage: 1917 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n",
                        "> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "# set Logging to DEBUG for more detailed outputs\n",
                "query_engine = index.as_query_engine()\n",
                "response = query_engine.query(\"What did the author do growing up?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "<b>\n",
                            "The author grew up writing short stories and programming on the IBM 1401. He also nagged his father to buy him a TRS-80 microcomputer, which he used to write simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but eventually switched to AI.</b>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display(Markdown(f\"<b>{response}</b>\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a91e2008",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
