{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever Router Query Engine\n",
    "In this tutorial, we define a router query engine based on a retriever. The retriever will select a set of nodes, and we will in turn select the right QueryEngine.\n",
    "\n",
    "NOTE: This is a beta feature. The interface for retrieving query engines may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes. \n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.  \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    GPTListIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.data_structs import Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize service context (set chunk size)\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define List Index and Vector Index over Same Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n"
     ]
    }
   ],
   "source": [
    "list_index = GPTListIndex(nodes, storage_context=storage_context)\n",
    "vector_index = GPTVectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Node/Query Engine for these Indices\n",
    "\n",
    "We define a Node and Query Engine for each Index. We then define an outer \"tool\" index to store\n",
    "these Nodes, which can be treated as metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_index_node = Node(\n",
    "    \"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
    "    doc_id=\"list_index\"\n",
    ")\n",
    "list_query_engine = list_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "vector_index_node = Node(\n",
    "    \"Useful for questions around the author's education, from Paul Graham essay on What I Worked On.\",\n",
    "    doc_id=\"vector_index\"\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Vector Index Retriever for these Nodes\n",
    "\n",
    "Define a vector index on top of these Nodes which in turn correspond to the underlying query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 40 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 40 tokens\n"
     ]
    }
   ],
   "source": [
    "# create an outer \"tool\" index to store the underlying index information\n",
    "tool_index = GPTVectorStoreIndex([list_index_node, vector_index_node])\n",
    "# get retriever\n",
    "tool_retriever = tool_index.as_retriever(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Router Query Engine\n",
    "\n",
    "We define a router query engine using the vector index retriever as input. This retriever will be used to retrieve \"Nodes\" which contain metadata for query engines. We also take as input a function that maps a Node to a query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_to_query_engine(node: Node):\n",
    "    \"\"\"Convert node to query engine.\"\"\"\n",
    "    # NOTE: hardcode mapping in this case\n",
    "    mapping = {\n",
    "        \"list_index\": list_query_engine,\n",
    "        \"vector_index\": vector_query_engine\n",
    "    }\n",
    "    return mapping[node.get_doc_id()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RetrieverRouterQueryEngine\n",
    "\n",
    "\n",
    "query_engine = RetrieverRouterQueryEngine(\n",
    "    tool_retriever,\n",
    "    node_to_query_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query('What is the summary of the document?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document is a narrative about the author's journey from writing short stories and programming on an IBM 1401 in high school to studying philosophy in college, discovering AI and Lisp, leaving his startup Viaweb to pursue painting, and eventually working on a new Lisp language called Bel. It discusses the advantages of being independent-minded in fields affected by rapid change, and looks at the history of Y Combinator and how it has helped to create more startups than would have otherwise existed. It also examines the concept of invented versus discovered, using the example of space aliens and McCarthy's Lisp to illustrate the idea that discoveredness can be preserved.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query('What did Paul Graham do during his time in college?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paul Graham did a variety of things during his time in college. He was a PhD student in computer science, but he also worked on other projects such as writing essays, working on Y Combinator, and writing a new version of Arc. He also took art classes at Harvard and applied to art schools such as RISD and the Accademia di Belli Arti in Florence. He eventually passed the entrance exam for the Accademia and left his PhD program to pursue art.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
